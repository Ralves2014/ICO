# Otimiza√ß√£o de Fun√ß√µes n√£o Lineares 

## Introdu√ß√£o
A otimiza√ß√£o de fun√ß√µes n√£o lineares √© um campo fundamental na matem√°tica aplicada e na ci√™ncia da computa√ß√£o, desempenhando um papel crucial na resolu√ß√£o de uma variedade de problemas complexos. O problema em quest√£o reside na minimiza√ß√£o de fun√ß√µes n√£o lineares, um desafio que permeia diversas √°reas do conhecimento, desde a modelagem de fen√≥menos f√≠sicos at√© otimiza√ß√£o de par√¢metros em aplica√ß√µes de machine learning.
As fun√ß√µes n√£o lineares podem representar uma ampla diversidade de fen√≥menos e processos da vida real, muitas vezes caracterizados por comportamentos n√£o triviais e m√∫ltiplos m√≠nimos locais. A busca pelo m√≠nimo global neste tipo de fun√ß√µes √© frequentemente complicada pela presen√ßa desses m√≠nimos locais, tornando o problema de otimiza√ß√£o n√£o linear altamente desafiador.
A complexidade destas fun√ß√µes e a presen√ßa de m√∫ltiplos m√≠nimos locais exigem m√©todos de otimiza√ß√£o eficazes que possam encontrar solu√ß√µes de forma r√°pida e precisa. Nesse contexto, os m√©todos de busca em linha emergem como ferramentas poderosas, explorando eficientemente o espa√ßo de busca para encontrar os m√≠nimos desejados.

## Contextualiza√ß√£o Te√≥rica
### M√©todos de Busca em Linha
Os m√©todos de busca em linha s√£o algoritmos que visam responder ao problema anteriormente identificado. Atrav√©s de v√°rias itera√ß√µes, procura-se atingir solu√ß√µes aproximadas ùë• ‚àó em problemas de minimiza√ß√£o ( ùëöùëñùëõùë• ùëì(ùë•) = ?).
A partir de um ponto inicial /semente, pretende-se, em cada passo, avan√ßar numa dire√ß√£o de descida, de forma a garantir que o valor seguinte √© sempre inferior ao anterior, com o objetivo de atingir o m√≠nimo. Neste trabalho abordar-se-√£o alguns m√©todos de busca diferentes, como o M√©todo de Descida M√°xima e o M√©todo de Newton-Raphson. Para al√©m disso, ser√° analisada a utiliza√ß√£o das condi√ß√µes de Wolfe.

### M√©todo de Descida M√°xima
O M√©todo de Descida M√°xima, tamb√©m conhecido como m√©todo do gradiente descendente, √© uma abordagem iterativa usada para encontrar o m√≠nimo de uma fun√ß√£o. Este m√©todo baseia-se na minimiza√ß√£o iterativa de uma fun√ß√£o, ajustando os par√¢metros na dire√ß√£o oposta ao gradiente local.
O gradiente indica a dire√ß√£o de maior inclina√ß√£o da fun√ß√£o, e ao mover-se na dire√ß√£o oposta, procura-se atingir o ponto de m√≠nimo da fun√ß√£o. A atualiza√ß√£o dos par√¢metros ocorre multiplicando o gradiente pela taxa de aprendizagem, controlando assim o tamanho dos passos em cada itera√ß√£o. Este m√©todo revela-se particularmente eficaz em problemas de otimiza√ß√£o convexa, proporcionando uma abordagem sistem√°tica para a converg√™ncia em dire√ß√£o ao m√≠nimo global.

### M√©todo de Newton-Raphson
O M√©todo de Newton-Raphson √© uma t√©cnica iterativa utilizada para encontrar ra√≠zes de fun√ß√µes ou extremos (m√≠nimos/m√°ximos). Esta abordagem fundamenta-se na aproxima√ß√£o local da fun√ß√£o por uma par√°bola, utilizando informa√ß√µes da fun√ß√£o e da sua segunda derivada (curvatura). Ao determinar o m√≠nimo ou m√°ximo desta par√°bola, obt√©m-se uma estimativa mais precisa da solu√ß√£o.
A atualiza√ß√£o dos par√¢metros leva em considera√ß√£o tanto o gradiente como a matriz Hessiana da fun√ß√£o. Este m√©todo permite uma converg√™ncia mais r√°pida em compara√ß√£o com o M√©todo de Descida M√°xima, uma vez que incorpora informa√ß√µes adicionais sobre a curvatura da fun√ß√£o. Contudo, o c√°lculo da matriz Hessiana pode ser computacionalmente exigente, e em alguns casos, a singularidade ou complexidade do problema pode afetar a efic√°cia do m√©todo.
